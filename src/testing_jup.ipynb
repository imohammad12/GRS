{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1.25}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications_Asset.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1.25}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications_Asset.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "sample\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['config'])\n",
    "from config import model_config as config\n",
    "import spacy\n",
    "from pattern.en import lexeme\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "config['gpu'] = 1\n",
    "# importlib.reload(sys.modules['config'])\n",
    "config['file_name'] = 'testing_paraphrasing.txt'\n",
    "open(config['file_name'], \"w\").close()\n",
    "\n",
    "from utils import *\n",
    "print(config['operation'])\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/m25dehgh/simplification/Edit-Unsup-TS/src\n",
      "/home/m25dehgh/anaconda3/envs/simpenv38/bin/python\n",
      "Reading lines...\n",
      "loading Wikilarge data\n",
      "Read 296402 train sentence pairs\n",
      "Read 1999 valid sentence pairs\n",
      "Read 358 test sentence pairs\n",
      "Read 237052 unique train sentence pairs\n",
      "Read 1999 unique valid sentence pairs\n",
      "Read 358 unique test sentence pairs\n",
      "Trimmed to 296402 train sentence pairs\n",
      "Trimmed to 1999 valid sentence pairs\n",
      "Trimmed to 358 test sentence pairs\n",
      "Trimmed to 237052 unique train sentence pairs\n",
      "Trimmed to 1999 unique valid sentence pairs\n",
      "Trimmed to 358 unique test sentence pairs\n",
      "Loading/Building vocabulary\n",
      "outputword2count (Vocab) file present\n",
      "Generating tf-idf file using simple sentences from the training set\n",
      "Calculating unigram probabilities\n",
      "OOV count\n",
      "1213\n",
      "Total vocabulary size:\n",
      "simple 35650\n"
     ]
    }
   ],
   "source": [
    "%cd ~/simplification/Edit-Unsup-TS/src/\n",
    "! which python\n",
    "\n",
    "from model.structural_decoder import DecoderGRU\n",
    "\n",
    "\n",
    "idf, unigram_prob, output_lang, tag_lang, dep_lang, train_simple, valid_simple, test_simple, train_complex, valid_complex, test_complex, output_embedding_weights, tag_embedding_weights, dep_embedding_weights = prepareData(config['embedding_dim'],\n",
    "config['freq'], config['ver'], config['dataset'], config['operation'])\n",
    "\n",
    "\n",
    "\n",
    "lm_forward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)\n",
    "lm_backward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1.25}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications_Asset_dl:1_par:1.75_rest:false.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': True, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': -10, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sys.modules['config'])\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ls': 0, 'dl': 0, 'las': 0, 'rl': 0, 'par': 0}\n",
      "Getting candidates for iteration:  0\n",
      "input sentence:  below are some useful links to facilitate your involvement .\n",
      "neg constraints:  facilitate facilitates facilitating facilitated\n",
      "input:  below are some useful links to facilitate your involvement .\tfacilitate|facilitates|facilitating|facilitated\t\n",
      "new:  Below are some useful links to make it easier for you to participate.\n",
      "\n",
      "Candidate: below are some useful links to facilitate .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 89.86506697962842\n",
      "Candidate: below are some useful links to .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 0.0\n",
      "Candidate: below are some useful links .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 0.0\n",
      "Candidate: below are some useful links .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 0.0\n",
      "Candidate: below are .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 0.0\n",
      "Candidate: below .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 0.0\n",
      "Candidate: are some useful links to facilitate your involvement .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 71.53771345723129\n",
      "Candidate: below are some useful links to make it easier for you to participate .\n",
      "Old Prob: 85.85683859700534, New Sent Prob: 0.0\n",
      "Getting candidates for iteration:  1\n",
      "input sentence:  below are some useful links to facilitate .\n",
      "neg constraints:  facilitate facilitates facilitating facilitated\n",
      "input:  below are some useful links to facilitate .\tfacilitate|facilitates|facilitating|facilitated\t\n",
      "new:  Below are some useful links for facilitation.\n",
      "\n",
      "Candidate: are some useful links to facilitate .\n",
      "Old Prob: 89.86506697962842, New Sent Prob: 0.0\n",
      "Candidate: below are some useful links for facilitation .\n",
      "Old Prob: 89.86506697962842, New Sent Prob: 0.0\n",
      "Input complex sentence\n",
      "below are some useful links to facilitate your involvement .\n",
      "Reference sentence\n",
      "below are some useful links to to help you get involved .\n",
      "Simplified sentence\n",
      "below are some useful links to facilitate .\n",
      "\n",
      "\n",
      "Average sentence level SARI till now for sentences\n",
      "0.6323713323713324\n",
      "0.8971139971139971\n",
      "1.0\n",
      "0.0\n",
      "Average sentence level BLEU till now for sentences\n",
      "0.4288819424803534\n",
      "Average perplexity of sentences\n",
      "89.86506697962842\n",
      "Average sentence level FKGL and FRE till now for sentences\n",
      "9.054285714285715\n",
      "42.61571428571432\n",
      "\n",
      "\n",
      "1\n",
      "Runtime of the program is 12.03732681274414\n",
      "total paraphrasing calls 2, total beam calls 2\n",
      "{'ls': 0, 'dl': 1, 'las': 0, 'rl': 0, 'par': 0}\n"
     ]
    }
   ],
   "source": [
    "import tree_edits_beam\n",
    "importlib.reload(sys.modules['tree_edits_beam'])\n",
    "from tree_edits_beam import *\n",
    "cmplx = [\"below are some useful links to facilitate your involvement .\"]\n",
    "simp = [\"below are some useful links to to help you get involved .\"]\n",
    "# cmplx = ['this was demonstrated in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# simp = ['this was shown in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# cmplx = [\"one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.\"]\n",
    "# simp = [\"On one side of the conflicts are the Sudanese military and the Janjaweed, a Sudanese militia group.  They are mostly recruited from the Afro-Arab Abbala tribes.\"] \n",
    "# cmplx = [\"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sample(cmplx, simp, output_lang, tag_lang, dep_lang, lm_forward, lm_backward, output_embedding_weights, idf, unigram_prob, start)\n",
    "\n",
    "# end = time.time()\n",
    "# print(f\"Runtime of the program is {end - start}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DebertaForSequenceClassification, Trainer, TrainingArguments, DebertaTokenizerFast\n",
    "\n",
    "root = \"/home/m25dehgh/simplification/complex-classifier\"\n",
    "model_name = \"newsela-auto-high-quality\"\n",
    "path_model = root + '/results' + '/' + model_name + \"/whole-high-quality/checkpoint-44361/\"\n",
    "comp_simp_class_model = DebertaForSequenceClassification.from_pretrained(path_model)\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser('http://localhost:9000')\n",
    "\n",
    "\n",
    "# sent = 'one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.'\n",
    "# sent = \"Jeddah is the principal gateway to Mecca, Islam's holiest city, which able-bodied Muslims are required to visit at least once in their lifetime.\"\n",
    "# sent = \"Since 2000, the recipient of the Kate Greenaway Medal has also been presented with the Colin Mears Award to the value of £5000.\"\n",
    "sent = \"It was originally thought that the debris thrown up by the collision filled in the smaller craters.\"\n",
    "# sent = \"The Great Dark Spot is thought to represent a hole in the methane cloud deck of Neptune.\" \n",
    "# sent = \"The great dark spot is thought to be a hole in the methane cloud deck of neptune\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"one side of the armed conflict consists mainly of the sudanese army and the janjaweed of one of the sudanese militia groups .\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"\n",
    "# sent = 'They are culturally akin to the coastal peoples of Papua New Guinea.'\n",
    "# sent = 'this was demonstrated in the miller-urey experiment by stanley Miller and Harold in 1953 .'\n",
    "# sent = 'this was shown in the Miller - Urey experiment by stanley Miller and Harold in 1953 .'\n",
    "sent = all_norms(sent)\n",
    "tree = next(parser.raw_parse(sent))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_out(model, tokenizer, sent):\n",
    "    \"returns a dict contaiting : attention mat for all layers, tokens of the input sent, complexity probability\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    toks = tokenizer(text = sent, truncation=True, padding=True, max_length = 100, return_tensors='pt')\n",
    "    \n",
    "    input_ids = toks['input_ids'].to(device)\n",
    "    attention_mask = toks['attention_mask'].to(device)\n",
    "    token_type_ids=toks['token_type_ids'].to(device)\n",
    "    output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_attentions = True, return_dict = True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "    attention = output.attentions\n",
    "    \n",
    "    out = {\"attention\": attention, \"tokens\": tokens, \"prob\": output.logits.squeeze().softmax(dim=0)[1].item()}\n",
    "    \n",
    "    return out\n",
    "\n",
    "def comp_extract(sent, comp_simp_class_model, tokenizer):\n",
    "    'extracting complex tokens from'\n",
    "    \n",
    "    out = get_model_out(comp_simp_class_model, tokenizer, sent)\n",
    "    attention = out['attention']\n",
    "    tokens = out['tokens']\n",
    "    prob = out[\"prob\"]\n",
    "\n",
    "    layer = 1\n",
    "    CLS_attended_tokens = attention[layer].sum(dim=1)[0][0].topk(10)\n",
    "    # [tokens[i] if for i in CLS_attended_tokens[1]], attention[layer].sum(dim=1)[0][0].topk(10)[0]\n",
    "\n",
    "    more_than_thresh = []\n",
    "    less_than_thresh = []\n",
    "    thresh = attention[layer].sum(dim=1)[0][0].mean() * 3/2\n",
    "\n",
    "    for i in range(len(CLS_attended_tokens[0])):\n",
    "        if CLS_attended_tokens[0][i] > thresh:\n",
    "            more_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "        else:\n",
    "            less_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "    \n",
    "    extracted_comps = {\"comp_toks\": more_than_thresh, \n",
    "                       \"not_comp_toks\": less_than_thresh,\n",
    "                       \"threshold\": thresh.item(),\n",
    "                       \"attention\": attention,\n",
    "                       'tokens': tokens,\n",
    "                       'prob': prob,\n",
    "                      }\n",
    "    return extracted_comps\n",
    "\n",
    "\n",
    "def word_from_toks(comp_toks, tokens):\n",
    "    '''returns words for negative constraints'''\n",
    "\n",
    "    # maximum number of accepted negative constraints\n",
    "    max_num_accepted_consts = 4\n",
    "    negs = []\n",
    "    special_toks = ['[SEP]', '[CLS]', '.', 'Ġ.']\n",
    "    \n",
    "    for tok in comp_toks:\n",
    "        \n",
    "        # Each token should be a word, not a part of word\n",
    "        if tok[0] == 'Ġ' and tok not in special_toks:\n",
    "        \n",
    "            # first word is usually selected mistakably so we do not pass it to the paraphraser\n",
    "            if tokens.index(tok) + 1 != len(tokens) and tokens.index(tok) != 1: \n",
    "        \n",
    "                # We want the token be single word, not a starting part of a word\n",
    "                if tokens[tokens.index(tok) + 1][0] == 'Ġ':\n",
    "                    negs.append(tok[1:])\n",
    "\n",
    "    new_neg = []\n",
    "    # adding all words with similar root \n",
    "    for tok in negs[:max_num_accepted_consts]:\n",
    "        new_neg += lexeme(tok)\n",
    "    \n",
    "    return new_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1.25}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'testing_paraphrasing.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': True, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.7, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.5, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexeme handled\n",
      "neg constraints:  debri debris debriing debried collision collisions collisioning collisioned\n",
      "input:  it was originally thought that the debris thrown up by the collision filled in the smaller craters .\tdebri|debris|debriing|debried|collision|collisions|collisioning|collisioned\t\n",
      "new:  Originally, it was thought that the rubble thrown up by the crash filled with smaller craters.\n",
      "\n",
      "originally , it was thought that the rubble thrown up by the crash filled with smaller craters .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(sent)\n",
    "entits = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entits.append(ent.text)\n",
    "    print(ent.text)\n",
    "    \n",
    "print(paraph(sent, \"\", entits, rest_pos_const=False))\n",
    "\n",
    "\n",
    "\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO:sockeye.utils] Sockeye version 1.18.85, commit c1b1da8dc154a87cde0c41476eac2c7633fe6f9a, path /home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/__init__.py\n",
      "[INFO:sockeye.utils] MXNet version 1.7.0, path /home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/mxnet/__init__.py\n",
      "[INFO:sockeye.utils] Command: /home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/translate.py -m /home/m25dehgh/simplification/improved-ParaBank-rewriter --json-input --output-type json --beam-size 20 --beam-prune 20 --batch-size 10 --device-ids 0 --disable-device-locking\n",
      "[INFO:sockeye.utils] Arguments: Namespace(avoid_list=None, batch_size=10, beam_prune=20.0, beam_search_stop='all', beam_size=20, bucket_width=10, checkpoints=None, chunk_size=None, config=None, device_ids=[0], disable_device_locking=True, ensemble_mode='linear', input=None, input_factors=None, json_input=True, length_penalty_alpha=1.0, length_penalty_beta=0.0, lock_dir='/tmp', loglevel='INFO', max_input_len=None, max_output_length_num_stds=2, models=['/home/m25dehgh/simplification/improved-ParaBank-rewriter'], nbest_size=1, output=None, output_type='json', override_dtype=None, quiet=False, restrict_lexicon=None, restrict_lexicon_topk=None, sample=None, seed=None, skip_topk=False, softmax_temperature=None, strip_unknown_words=False, sure_align_threshold=0.9, use_cpu=False)\n",
      "[INFO:sockeye.utils] Attempting to acquire 1 GPUs of 3 GPUs.\n",
      "[INFO:__main__] Translate Device: gpu(0)\n",
      "[INFO:sockeye.inference] Loading 1 model(s) from ['/home/m25dehgh/simplification/improved-ParaBank-rewriter'] ...\n",
      "[INFO:sockeye.vocab] Vocabulary (19581 words) loaded from \"/home/m25dehgh/simplification/improved-ParaBank-rewriter/vocab.src.0.json\"\n",
      "[INFO:sockeye.vocab] Vocabulary (8 words) loaded from \"/home/m25dehgh/simplification/improved-ParaBank-rewriter/vocab.src.1.json\"\n",
      "[INFO:sockeye.vocab] Vocabulary (6 words) loaded from \"/home/m25dehgh/simplification/improved-ParaBank-rewriter/vocab.src.2.json\"\n",
      "[INFO:sockeye.vocab] Vocabulary (19581 words) loaded from \"/home/m25dehgh/simplification/improved-ParaBank-rewriter/vocab.trg.0.json\"\n",
      "[INFO:sockeye.inference] Model version: 1.18.57\n",
      "[INFO:sockeye.model] ModelConfig loaded from \"/home/m25dehgh/simplification/improved-ParaBank-rewriter/config\"\n",
      "[INFO:sockeye.inference] Disabling dropout layers for performance reasons\n",
      "[INFO:sockeye.model] Config[_frozen=True, config_data=Config[_frozen=True, data_statistics=Config[_frozen=True, average_len_target_per_bucket=[7.145073548898002, 13.99563686270617, 23.694915053093826, 33.6462227137744, 43.49996592832752, 53.29272556844158, 63.10734061789328, 72.93918370676286, 82.74942670991013, 91.22460162046973], buckets=[(10, 10), (20, 20), (30, 30), (40, 40), (50, 50), (60, 60), (70, 70), (80, 80), (90, 90), (100, 100)], length_ratio_mean=1.0442173528795242, length_ratio_std=0.22483564504779635, max_observed_len_source=100, max_observed_len_target=100, num_discarded=19307, num_sents=141362580, num_sents_per_bucket=[33253319, 53201168, 25616295, 13164610, 7278774, 4097681, 2341574, 1372938, 757906, 278315], num_tokens_source=2797831244, num_tokens_target=2903116929, num_unks_source=0, num_unks_target=0, size_vocab_source=19581, size_vocab_target=19581], max_seq_len_source=100, max_seq_len_target=100, num_source_factors=3, source_with_eos=True], config_decoder=Config[_frozen=True, act_type=relu, attention_heads=8, conv_config=None, dropout_act=0.0, dropout_attention=0.0, dropout_prepost=0.0, dtype=float32, feed_forward_num_hidden=2048, lhuc=False, max_seq_len_source=100, max_seq_len_target=100, model_size=512, num_layers=6, positional_embedding_type=fixed, postprocess_sequence=dr, preprocess_sequence=n, use_lhuc=False], config_embed_source=Config[_frozen=True, dropout=0.0, dtype=float32, factor_configs=[Config[_frozen=False, num_embed=4, vocab_size=8], Config[_frozen=False, num_embed=4, vocab_size=6]], num_embed=512, num_factors=3, source_factors_combine=concat, vocab_size=19581], config_embed_target=Config[_frozen=True, dropout=0.0, dtype=float32, factor_configs=None, num_embed=512, num_factors=1, source_factors_combine=concat, vocab_size=19581], config_encoder=Config[_frozen=True, act_type=relu, attention_heads=8, conv_config=None, dropout_act=0.0, dropout_attention=0.0, dropout_prepost=0.0, dtype=float32, feed_forward_num_hidden=2048, lhuc=False, max_seq_len_source=100, max_seq_len_target=100, model_size=520, num_layers=6, positional_embedding_type=fixed, postprocess_sequence=dr, preprocess_sequence=n, use_lhuc=False], config_loss=Config[_frozen=True, label_smoothing=0.1, name=cross-entropy, normalization_type=valid, vocab_size=19581], lhuc=False, vocab_source_size=19581, vocab_target_size=19581, weight_normalization=False, weight_tying=True, weight_tying_type=src_trg_softmax]\n",
      "[INFO:sockeye.encoder] sockeye.encoder.EncoderSequence dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.AddSinCosPositionalEmbeddings dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.TransformerEncoder dtype: float32\n",
      "[INFO:sockeye.decoder] sockeye.decoder.TransformerDecoder dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.AddSinCosPositionalEmbeddings dtype: float32\n",
      "[INFO:sockeye.model] Tying the source and target embeddings.\n",
      "[INFO:sockeye.model] Tying the target embeddings and output layer parameters.\n",
      "[INFO:sockeye.encoder] sockeye.encoder.Embedding dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.Embedding dtype: float32\n",
      "[INFO:sockeye.model] Loaded params from \"/home/m25dehgh/simplification/improved-ParaBank-rewriter/params.best\"\n",
      "[INFO:sockeye.inference] 1 model(s) loaded in 2.6103s\n",
      "[INFO:sockeye.inference] Translator (1 model(s) beam_size=20 beam_prune=20.00 beam_search_stop=all nbest_size=1 ensemble_mode=None max_batch_size=10 buckets_source=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100] avoiding=0)\n",
      "[INFO:__main__] Translating...\n",
      "[ERROR:root] Uncaught exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/translate.py\", line 228, in <module>\n",
      "    main()\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/translate.py\", line 41, in main\n",
      "    run_translate(args)\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/translate.py\", line 113, in run_translate\n",
      "    read_and_translate(translator=translator,\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/translate.py\", line 196, in read_and_translate\n",
      "    chunk_time = translate(output_handler, chunk, translator)\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/translate.py\", line 219, in translate\n",
      "    trans_outputs = translator.translate(trans_inputs)\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/inference.py\", line 1463, in translate\n",
      "    batch_translations = self._translate_nd(*self._get_inference_input(translator_inputs))\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/inference.py\", line 1615, in _translate_nd\n",
      "    return self._get_best_from_beam(*self._beam_search(source,\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/inference.py\", line 1905, in _beam_search\n",
      "    avoid_states.reorder(best_hyp_indices)\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/sockeye/lexical_constraints.py\", line 192, in reorder\n",
      "    self.local_avoid_states = [self.local_avoid_states[x] for x in indices.asnumpy()]\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py\", line 2563, in asnumpy\n",
      "    check_call(_LIB.MXNDArraySyncCopyToCPU(\n",
      "  File \"/home/m25dehgh/anaconda3/envs/IMR/lib/python3.8/site-packages/mxnet/base.py\", line 246, in check_call\n",
      "    raise get_last_ffi_error()\n",
      "mxnet.base.MXNetError: Traceback (most recent call last):\n",
      "  File \"src/storage/./pooled_storage_manager.h\", line 161\n",
      "MXNetError: cudaMalloc retry failed: out of memory\n"
     ]
    }
   ],
   "source": [
    "! /home/m25dehgh/simplification/improved-ParaBank-rewriter/paraphrase.sh < ./inp_par.txt > ./out_par.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.']\n",
      "===========\n",
      "['neptune']\n",
      "===========\n",
      "['neptune']\n",
      "===========\n",
      "['of']\n",
      "===========\n",
      "['of', 'neptune']\n",
      "===========\n",
      "['deck']\n",
      "===========\n",
      "['cloud']\n",
      "===========\n",
      "['methane']\n",
      "===========\n",
      "['the']\n",
      "===========\n",
      "['the', 'methane', 'cloud', 'deck']\n",
      "===========\n",
      "['the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['in']\n",
      "===========\n",
      "['in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['hole']\n",
      "===========\n",
      "['a']\n",
      "===========\n",
      "['a', 'hole']\n",
      "===========\n",
      "['represent']\n",
      "===========\n",
      "['represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['to']\n",
      "===========\n",
      "['to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['thought']\n",
      "===========\n",
      "['thought', 'to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['is']\n",
      "===========\n",
      "['is', 'thought', 'to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['spot']\n",
      "===========\n",
      "['dark']\n",
      "===========\n",
      "['great']\n",
      "===========\n",
      "['the']\n",
      "===========\n",
      "['the', 'great', 'dark', 'spot']\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pos) - 1, 1, -1):\n",
    "    if not isinstance(tree[pos[i]], str):\n",
    "        print(tree[pos[i]].leaves())\n",
    "        print(\"===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1c43965fc138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreepositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "pos = tree.treepositions()\n",
    "len(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phrase_tags = ['S', 'ADJP', 'ADVP', 'CONJP', 'FRAG', 'INTJ', 'LST', 'NAC', 'NP', 'NX', 'PP', 'PRN', 'PRT',\n",
    "               'QP', 'RRC', 'UCP', 'VP', 'WHADJP', 'WHAVP', 'WHNP', 'WHPP', 'X', 'SBAR']\n",
    "p = []\n",
    "pos = tree.treepositions()\n",
    "for i in range(len(pos) - 1, 1, -1):\n",
    "    if not isinstance(tree[pos[i]], str):\n",
    "        if tree[pos[i]].label() in phrase_tags:\n",
    "            p.append(tree[pos[i]].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['neptune'],\n",
       " ['of', 'neptune'],\n",
       " ['the', 'methane', 'cloud', 'deck'],\n",
       " ['the', 'methane', 'cloud', 'deck', 'of', 'neptune'],\n",
       " ['in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune'],\n",
       " ['a', 'hole'],\n",
       " ['represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['thought',\n",
       "  'to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['is',\n",
       "  'thought',\n",
       "  'to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['the', 'great', 'dark', 'spot']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in p:\n",
    "#     # print(\"p[i] is :\", i)\n",
    "#     print(\"out put for dlt :\", delete_leaves(sent, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', 'useful', 'links', 'to', 'facilitate', 'your', 'involvement']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9b9e9d5a5d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "i =8\n",
    "if 2 <= len(p[i]) <= 7:\n",
    "    print(p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ['to',\n",
    "  'represent',\n",
    "  'a',\n",
    "  'hole',\n",
    "  'in',\n",
    "  'the',\n",
    "  'methane',\n",
    "  'cloud',\n",
    "  'deck',\n",
    "  'of',\n",
    "  'neptune']\n",
    "a = ['to',\n",
    "  'represent',\n",
    "  'a',\n",
    "  'hole',\n",
    "  'in',\n",
    "  'the',\n",
    "  'methane',\n",
    "  'cloud',\n",
    "  'deck',\n",
    "  'of',\n",
    "  'neptune']\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(sent)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def const_paraph(sent, neg_const, rest_pos_const=False):\n",
    "    stp_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    neg_const = neg_const.split(\" \")\n",
    "    entities = get_entities(sent)\n",
    "    pos_const = []\n",
    "\n",
    "    neg_const = [x for x in neg_const if x not in entities and x not in stp_words]\n",
    "\n",
    "    if len(neg_const) >= 10:\n",
    "        return -1\n",
    "\n",
    "    if rest_pos_const:\n",
    "        for i in sent.split():\n",
    "            if i not in neg_const and i not in stp_words:\n",
    "                pos_const.append(i.lower())\n",
    "    else:\n",
    "        pos_const = entities\n",
    "\n",
    "    inp = sent + '.' + \"\\t\" + \"|\".join(neg_const) + '\\t' + \"|\".join(pos_const)\n",
    "\n",
    "    print(\"input: \", inp)\n",
    "\n",
    "    f = open(\"inp_par.txt\", \"w\")\n",
    "    f.write(inp)\n",
    "    f.close()\n",
    "\n",
    "    f = open(\"out_par.txt\", \"w\")\n",
    "    f.close()\n",
    "\n",
    "    #TODO\n",
    "    imr_dir_path = '/home/m25dehgh/simplification/improved-ParaBank-rewriter'\n",
    "    bashCommand = f\"{imr_dir_path}/paraphrase.sh < ./inp_par.txt > ./out_par.txt \"\n",
    "\n",
    "    os.system(bashCommand)\n",
    "\n",
    "    f = open(\"out_par.txt\", \"r\")\n",
    "    return f.read()\n",
    "\n",
    "# def cons_paraphrase(sent, leaves):\n",
    "def paraph(sent, leaves, rest_pos_const=False):\n",
    "#     phrase = ''\n",
    "    # print(leaves)\n",
    "#     for i in range(len(leaves)):\n",
    "#         phrase = phrase + ' ' + leaves[i]\n",
    "#     phrase = phrase + ' '\n",
    "#     old = sent\n",
    "    # print(\"deleting\", phrase)\n",
    "\n",
    "    sent = const_paraph(sent, \" \".join(leaves), rest_pos_const)\n",
    "\n",
    "#     print('new: ', sent)\n",
    "    return correct(all_norms(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  below are some useful links to facilitate your involvement .\tuseful|links|facilitate|involvement\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'below are some helpful references to make it easier for you to participate .'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraph(sent, p[4], rest_pos_const=False)\n",
    "# sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since 2000  the recipient of the kate greenaway medal has also been presented with the colin mears award to the value of £ 5000 .\tpresented|value\tsince|2000|recipient|kate|greenaway|medal|also|colin|mears|award|£|5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'since 2000 kate greenaway medal recipient also award colin mears 5000 <unk>.\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_paraph(all_norms(sent), \"been presented to the value of 5000\", rest_pos_const=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phrase = ''\n",
    "# print(leaves)\n",
    "for i in range(len(p[12])):\n",
    "    phrase = phrase + ' ' + p[12][i]\n",
    "phrase = phrase + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a sudanese militia group'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(p[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a sudanese militia group '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpenv38",
   "language": "python",
   "name": "simpenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
