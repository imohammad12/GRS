{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "sample\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['config'])\n",
    "from config import model_config as config\n",
    "import spacy\n",
    "from pattern.en import lexeme\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "config['gpu'] = 1\n",
    "# importlib.reload(sys.modules['config'])\n",
    "config['file_name'] = 'testing_paraphrasing.txt'\n",
    "open(config['file_name'], \"w\").close()\n",
    "\n",
    "from utils import *\n",
    "print(config['operation'])\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/m25dehgh/simplification/Edit-Unsup-TS/src\n",
      "/home/m25dehgh/anaconda3/envs/simpenv38/bin/python\n",
      "Reading lines...\n",
      "loading Wikilarge data\n",
      "Read 296402 train sentence pairs\n",
      "Read 1999 valid sentence pairs\n",
      "Read 358 test sentence pairs\n",
      "Read 237052 unique train sentence pairs\n",
      "Read 1999 unique valid sentence pairs\n",
      "Read 358 unique test sentence pairs\n",
      "Trimmed to 296402 train sentence pairs\n",
      "Trimmed to 1999 valid sentence pairs\n",
      "Trimmed to 358 test sentence pairs\n",
      "Trimmed to 237052 unique train sentence pairs\n",
      "Trimmed to 1999 unique valid sentence pairs\n",
      "Trimmed to 358 unique test sentence pairs\n",
      "Loading/Building vocabulary\n",
      "outputword2count (Vocab) file present\n",
      "Generating tf-idf file using simple sentences from the training set\n",
      "Calculating unigram probabilities\n",
      "OOV count\n",
      "1213\n",
      "Total vocabulary size:\n",
      "simple 35650\n"
     ]
    }
   ],
   "source": [
    "%cd ~/simplification/Edit-Unsup-TS/src/\n",
    "! which python\n",
    "\n",
    "from model.structural_decoder import DecoderGRU\n",
    "\n",
    "\n",
    "idf, unigram_prob, output_lang, tag_lang, dep_lang, train_simple, valid_simple, test_simple, train_complex, valid_complex, test_complex, output_embedding_weights, tag_embedding_weights, dep_embedding_weights = prepareData(config['embedding_dim'],\n",
    "config['freq'], config['ver'], config['dataset'], config['operation'])\n",
    "\n",
    "\n",
    "\n",
    "lm_forward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)\n",
    "lm_backward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1.1, 'las': 3.0, 'rl': 1.25, 'par': 1.1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': True, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sys.modules['config'])\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ls': 0, 'dl': 0, 'las': 0, 'rl': 0, 'par': 0}\n",
      "similarity of the two sentences:  tensor(1., device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9954376816749573\n",
      "Getting candidates for iteration:  0\n",
      "neg constraints:  facilitate facilitates facilitating facilitated facilitator facilitative facilitation\n",
      "input:  below are some useful links to facilitate your involvement .\tfacilitate|facilitates|facilitating|facilitated|facilitator|facilitative|facilitation\t\n",
      "new:  Below are some useful links to make it easier for you to participate.\n",
      "\n",
      "similarity of the two sentences:  tensor(0.7492, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.7254489064216614\n",
      "Candidate: below are some useful links to make it easier for you to .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7301, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9946063160896301\n",
      "Candidate: below are some useful links to make it easier for you .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7301, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9946063160896301\n",
      "Candidate: below are some useful links to make it easier for you .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8370, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.14370258152484894\n",
      "Candidate: below are some useful links to make it easier for to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9938867688179016\n",
      "Candidate: below are some useful links to make it easier to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.17914056777954102 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8609, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.5672221183776855\n",
      "Candidate: below are some useful links to make it to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7908, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.929359495639801\n",
      "Candidate: below are some useful links to make it .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8563, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.984474778175354\n",
      "Candidate: below are some useful links to make easier for you to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.14753645658493042 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8520, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.05085594579577446\n",
      "Candidate: below are some useful links to easier for you to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8662, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.6153947114944458\n",
      "Candidate: below are some useful links easier for you to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8662, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.6153947114944458\n",
      "Candidate: below are some useful links easier for you to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.5648, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.12775777280330658\n",
      "Candidate: below are easier for you to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8413, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9107824563980103\n",
      "Candidate: are some useful links to make it easier for you to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9938867688179016\n",
      "Getting candidates for iteration:  1\n",
      "neg constraints:  participate participates participating participated participant participance participancy participator participable participantly participation participative participability participatingly participatively link links linking linked link linked linking\n",
      "input:  below are some useful links to make it easier to participate .\tparticipate|participates|participating|participated|participant|participance|participancy|participator|participable|participantly|participation|participative|participability|participatingly|participatively|link|links|linking|linked|link|linked|linking\t\n",
      "new:  Below are some useful references to make it easier to take part in this process.\n",
      "\n",
      "similarity of the two sentences:  tensor(0.7929, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9266795516014099\n",
      "Candidate: below are some useful references to make it easier to take part in .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7980, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9926226139068604\n",
      "Candidate: below are some useful references to make it easier to take part .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0.4749264717102051 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7031, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.976241409778595\n",
      "Candidate: below are some useful references to make it easier to take in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.6981, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.37045371532440186\n",
      "Candidate: below are some useful references to make it easier to .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7043, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9929171800613403\n",
      "Candidate: below are some useful references to make it easier .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7043, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9929171800613403\n",
      "Candidate: below are some useful references to make it easier .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7772, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.2211207002401352\n",
      "Candidate: below are some useful references to make it to take part in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7145, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.8506163358688354\n",
      "Candidate: below are some useful references to make it .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7681, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.8403657078742981\n",
      "Candidate: below are some useful references to make easier to take part in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7418, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.09428820013999939\n",
      "Candidate: below are some useful references to easier to take part in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7587, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.22778913378715515\n",
      "Candidate: below are some useful references easier to take part in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7587, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.22778913378715515\n",
      "Candidate: below are some useful references easier to take part in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.5311, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.13385868072509766\n",
      "Candidate: below are easier to take part in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.7423, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.912691593170166\n",
      "Candidate: are some useful references to make it easier to take part in this process .\n",
      "Old Prob: 0.17914056777954102, New Sent Prob: 0 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity of the two sentences:  tensor(0.7980, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9926226139068604\n",
      "Getting candidates for iteration:  2\n",
      "neg constraints:  reference references referencing referenced refer referent referable referment reference referently\n",
      "input:  below are some useful references to make it easier to take part .\treference|references|referencing|referenced|refer|referent|referable|referment|reference|referently\t\n",
      "new:  Below are some useful links to facilitate participation.\n",
      "\n",
      "similarity of the two sentences:  tensor(0.9078, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.8696845173835754\n",
      "Candidate: below are some useful links to facilitate .\n",
      "Old Prob: 0.4749264717102051, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8438, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.21995963156223297\n",
      "Candidate: below are some useful links to .\n",
      "Old Prob: 0.4749264717102051, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9956685304641724\n",
      "Candidate: below are some useful links .\n",
      "Old Prob: 0.4749264717102051, New Sent Prob: 0.8880444020032883 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9956685304641724\n",
      "Candidate: below are some useful links .\n",
      "Old Prob: 0.4749264717102051, New Sent Prob: 0.8880444020032883 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.3400, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.6634459495544434\n",
      "Candidate: below are .\n",
      "Old Prob: 0.4749264717102051, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.2834, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.7245055437088013\n",
      "Candidate: below .\n",
      "Old Prob: 0.4749264717102051, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.9039, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9350395202636719\n",
      "Candidate: are some useful links to facilitate participation .\n",
      "Old Prob: 0.4749264717102051, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9956685304641724\n",
      "Getting candidates for iteration:  3\n",
      "neg constraints:  \n",
      "input:  below are some useful links .\t\t\n",
      "new:  There are some useful links below.\n",
      "\n",
      "similarity of the two sentences:  tensor(0.8171, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9937792420387268\n",
      "Candidate: there are some useful links .\n",
      "Old Prob: 0.8880444020032883, New Sent Prob: 0.8877910897135735 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.3584, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.5981194376945496\n",
      "Candidate: there are below .\n",
      "Old Prob: 0.8880444020032883, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.1889, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.2954772412776947\n",
      "Candidate: there .\n",
      "Old Prob: 0.8880444020032883, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8347, device='cuda:0')\n",
      "candidate sentence grammar validity probability:  0.9460943341255188\n",
      "Candidate: are some useful links below .\n",
      "Old Prob: 0.8880444020032883, New Sent Prob: 0 \n",
      "\n",
      "Input complex sentence\n",
      "below are some useful links to facilitate your involvement .\n",
      "Reference sentence\n",
      "below are some useful links to to help you get involved .\n",
      "Simplified sentence\n",
      "below are some useful links .\n",
      "\n",
      "\n",
      "Average sentence level SARI till now for sentences\n",
      "0.5515923890923892\n",
      "0.8672771672771673\n",
      "0.7875000000000001\n",
      "0.0\n",
      "Average sentence level BLEU till now for sentences\n",
      "0.2925637512788283\n",
      "Average perplexity of sentences\n",
      "0.8880444020032883\n",
      "Average sentence level FKGL and FRE till now for sentences\n",
      "5.240000000000002\n",
      "66.40000000000003\n",
      "\n",
      "\n",
      "1\n",
      "Runtime of the program is 70.34490013122559\n",
      "total paraphrasing calls 0, total beam calls 4\n",
      "{'ls': 0, 'dl': 5, 'las': 0, 'rl': 0, 'par': 0}\n"
     ]
    }
   ],
   "source": [
    "import tree_edits_beam\n",
    "importlib.reload(sys.modules['tree_edits_beam'])\n",
    "from tree_edits_beam import *\n",
    "cmplx = [\"below are some useful links to facilitate your involvement .\"]\n",
    "simp = [\"below are some useful links to to help you get involved .\"]\n",
    "# cmplx = ['this was demonstrated in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# simp = ['this was shown in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# cmplx = [\"one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.\"]\n",
    "# simp = [\"On one side of the conflicts are the Sudanese military and the Janjaweed, a Sudanese militia group.  They are mostly recruited from the Afro-Arab Abbala tribes.\"] \n",
    "# cmplx = [\"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"]\n",
    "# cmplx = ['The Kindle 2 features 16-level grayscale display, improved battery life, 20 percent faster page-refreshing, a text-to-speech option to read the text aloud, and overall thickness reduced from 0.8 to 0.36 inches (9.1 millimeters).']\n",
    "# simp = ['The Kindle 2 features several technological advances.']\n",
    "\n",
    "\n",
    "# cmplx = ['before the advent of the pocket calculator it was the most commonly used calculation tool in science and engineering .']\n",
    "# simp = ['It was the most commonly used calculation tool before the invention of the pocket calculator.']\n",
    "# cmplx = ['furthermore, she appeared in several music videos, including \"it girl\" by john oates and \"just lose it\" by eminem.']\n",
    "# simp = ['she was in music videos, including \"it girl\" by john oates and \"just lose it\" by eminem.']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# cmplx[0] = all_norms(cmplx[0])\n",
    "\n",
    "sample(cmplx, simp, output_lang, tag_lang, dep_lang, lm_forward, lm_backward, output_embedding_weights, idf, unigram_prob, start)\n",
    "\n",
    "# end = time.time()\n",
    "# print(f\"Runtime of the program is {end - start}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "below are some useful links to facilitate your involvement . \t\t below are some useful links to to help you get involved . \t\t Score: 0.9086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9086, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_sim(cmplx[0], simp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DebertaForSequenceClassification, Trainer, TrainingArguments, DebertaTokenizerFast\n",
    "\n",
    "root = \"/home/m25dehgh/simplification/complex-classifier\"\n",
    "model_name = \"newsela-auto-high-quality\"\n",
    "path_model = root + '/results' + '/' + model_name + \"/whole-high-quality/checkpoint-44361/\"\n",
    "comp_simp_class_model = DebertaForSequenceClassification.from_pretrained(path_model)\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'before the advent of the pocket calculator it was the most commonly used calculation tool in science and engineering .'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser('http://localhost:9000')\n",
    "\n",
    "\n",
    "# sent = 'one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.'\n",
    "# sent = \"Jeddah is the principal gateway to Mecca, Islam's holiest city, which able-bodied Muslims are required to visit at least once in their lifetime.\"\n",
    "# sent = \"Since 2000, the recipient of the Kate Greenaway Medal has also been presented with the Colin Mears Award to the value of £5000.\"\n",
    "sent = \"It was originally thought that the debris thrown up by the collision filled in the smaller craters.\"\n",
    "# sent = \"The Great Dark Spot is thought to represent a hole in the methane cloud deck of Neptune.\" \n",
    "# sent = \"The great dark spot is thought to be a hole in the methane cloud deck of neptune\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"one side of the armed conflict consists mainly of the sudanese army and the janjaweed of one of the sudanese militia groups .\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"\n",
    "# sent = 'They are culturally akin to the coastal peoples of Papua New Guinea.'\n",
    "# sent = 'this was demonstrated in the miller-urey experiment by stanley Miller and Harold in 1953 .'\n",
    "# sent = 'this was shown in the Miller - Urey experiment by stanley Miller and Harold in 1953 .'\n",
    "sent = 'Kindle 2 characteristics 16 levels of grayscale enhanced 20 faster page refurbishment of text for speech reading the text aloud and overall thickness reduced from 08 to 036 inches of lrb 91 mm rrb.'\n",
    "sent = 'furthermore, she appeared in several music videos, including \"it girl\" by john oates and \"just lose it\" by eminem.'\n",
    "sent = 'before the advent of the pocket calculator it was the most commonly used calculation tool in science and engineering .'\n",
    "# sent = 'Before proceeding with the pocket calculator, it was the most commonly used computation tool in science and engineering'\n",
    "\n",
    "sent = all_norms(sent)\n",
    "tree = next(parser.raw_parse(sent))\n",
    "sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_out(model, tokenizer, sent):\n",
    "    \"returns a dict contaiting : attention mat for all layers, tokens of the input sent, complexity probability\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    toks = tokenizer(text = sent, truncation=True, padding=True, max_length = 100, return_tensors='pt')\n",
    "    \n",
    "    input_ids = toks['input_ids'].to(device)\n",
    "    attention_mask = toks['attention_mask'].to(device)\n",
    "    token_type_ids=toks['token_type_ids'].to(device)\n",
    "    output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_attentions = True, return_dict = True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "    attention = output.attentions\n",
    "    \n",
    "    out = {\"attention\": attention, \"tokens\": tokens, \"prob\": output.logits.squeeze().softmax(dim=0)[1].item()}\n",
    "    \n",
    "    return out\n",
    "\n",
    "def comp_extract(sent, comp_simp_class_model, tokenizer):\n",
    "    'extracting complex tokens from'\n",
    "    \n",
    "    out = get_model_out(comp_simp_class_model, tokenizer, sent)\n",
    "    attention = out['attention']\n",
    "    tokens = out['tokens']\n",
    "    prob = out[\"prob\"]\n",
    "\n",
    "    layer = 1\n",
    "    CLS_attended_tokens = attention[layer].sum(dim=1)[0][0].topk(10)\n",
    "    # [tokens[i] if for i in CLS_attended_tokens[1]], attention[layer].sum(dim=1)[0][0].topk(10)[0]\n",
    "\n",
    "    more_than_thresh = []\n",
    "    less_than_thresh = []\n",
    "    thresh = attention[layer].sum(dim=1)[0][0].mean() * 3/2\n",
    "\n",
    "    for i in range(len(CLS_attended_tokens[0])):\n",
    "        if CLS_attended_tokens[0][i] > thresh:\n",
    "            more_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "        else:\n",
    "            less_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "    \n",
    "    extracted_comps = {\"comp_toks\": more_than_thresh, \n",
    "                       \"not_comp_toks\": less_than_thresh,\n",
    "                       \"threshold\": thresh.item(),\n",
    "                       \"attention\": attention,\n",
    "                       'tokens': tokens,\n",
    "                       'prob': prob,\n",
    "                      }\n",
    "    return extracted_comps\n",
    "\n",
    "\n",
    "def word_from_toks(comp_toks, tokens):\n",
    "    '''returns words for negative constraints'''\n",
    "\n",
    "    # maximum number of accepted negative constraints\n",
    "    max_num_accepted_consts = 4\n",
    "    negs = []\n",
    "    special_toks = ['[SEP]', '[CLS]', '.', 'Ġ.']\n",
    "    \n",
    "    for tok in comp_toks:\n",
    "        \n",
    "        # Each token should be a word, not a part of word\n",
    "        if tok[0] == 'Ġ' and tok not in special_toks:\n",
    "        \n",
    "            # first word is usually selected mistakably so we do not pass it to the paraphraser\n",
    "            if tokens.index(tok) + 1 != len(tokens) and tokens.index(tok) != 1: \n",
    "        \n",
    "                # We want the token be single word, not a starting part of a word\n",
    "                if tokens[tokens.index(tok) + 1][0] == 'Ġ':\n",
    "                    negs.append(tok[1:])\n",
    "\n",
    "    new_neg = []\n",
    "    # adding all words with similar root \n",
    "    for tok in negs[:max_num_accepted_consts]:\n",
    "        new_neg += lexeme(tok)\n",
    "    \n",
    "    return new_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1.1, 'las': 3.0, 'rl': 1.25, 'par': 1.1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': True, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg constraints:  advent advents adventing advented Advent adventive Adventism calculator calculators calculatorring calculatorred calculate calculous calculated calculable calculation calculative calculating calculatedly calculatingly calculability calculational\n",
      "input:  before the advent of the pocket calculator it was the most commonly used calculation tool in science and engineering .\tadvent|advents|adventing|advented|Advent|adventive|Adventism|calculator|calculators|calculatorring|calculatorred|calculate|calculous|calculated|calculable|calculation|calculative|calculating|calculatedly|calculatingly|calculability|calculational\t\n",
      "new:  It was the most commonly used computation tool in science and engineering.\n",
      "\n",
      "it was the most commonly used computation tool in science and engineering .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(sent)\n",
    "entits = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entits.append(ent.text)\n",
    "    print(ent.text)\n",
    "    \n",
    "print(paraph(sent, \"advent\", entits, stemmer, rest_pos_const=False))\n",
    "\n",
    "\n",
    "\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phrase_tags = ['S', 'ADJP', 'ADVP', 'CONJP', 'FRAG', 'INTJ', 'LST', 'NAC', 'NP', 'NX', 'PP', 'PRN', 'PRT',\n",
    "               'QP', 'RRC', 'UCP', 'VP', 'WHADJP', 'WHAVP', 'WHNP', 'WHPP', 'X', 'SBAR']\n",
    "p = []\n",
    "pos = tree.treepositions()\n",
    "for i in range(len(pos) - 1, 1, -1):\n",
    "    if not isinstance(tree[pos[i]], str):\n",
    "        if tree[pos[i]].label() in phrase_tags:\n",
    "            p.append(tree[pos[i]].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg constraints:  advent proceeding\n",
      "input:  before the advent of the pocket calculator it was the most commonly used calculation tool in science and engineering .\tadvent|proceeding\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Before the advance of the pocket calculator, it was the most commonly used computation tool in science and engineering.\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_paraph(all_norms(sent), [\"advent\", \"proceeding\"] , entits, rest_pos_const=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235886"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is taken from https://gist.github.com/wassname/7fd4c975883074a99864\n",
    "\n",
    "from collections import defaultdict\n",
    "class SnowCastleStemmer(nltk.stem.SnowballStemmer):\n",
    "    \"\"\" A wrapper around snowball stemmer with a reverse lookip table \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(self.__class__, self).__init__(*args, **kwargs)\n",
    "        self._stem_memory = defaultdict(set)\n",
    "        # switch stem and memstem\n",
    "        self._stem=self.stem\n",
    "        self.stem=self.memstem\n",
    "        \n",
    "    def memstem(self, word):\n",
    "        \"\"\" Wrapper around stem that remembers \"\"\"\n",
    "        stemmed_word = self._stem(word)\n",
    "        self._stem_memory[stemmed_word].add(word)\n",
    "        return stemmed_word\n",
    "        \n",
    "    def unstem(self, stemmed_word):\n",
    "        \"\"\" Reverse lookup \"\"\"\n",
    "        return sorted(self._stem_memory[stemmed_word], key=len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build', 'builds', 'building']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer= SnowCastleStemmer('english')\n",
    "stemmer.stem(\"building\")\n",
    "stemmer.stem(\"build\")\n",
    "stemmer.stem(\"builds\")\n",
    "stemmer.unstem(\"build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235886/235886 [00:02<00:00, 90000.49it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = \"facilitate\"\n",
    "a = stemmer.unstem(stemmer.stem(tok)).remove(tok)\n",
    "# a.remove(tok)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['facilitate', 'facilitates', 'facilitating', 'facilitated']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme(\"facilitate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpenv38",
   "language": "python",
   "name": "simpenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
