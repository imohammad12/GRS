{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "sample\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['config'])\n",
    "from config import model_config as config\n",
    "import spacy\n",
    "from pattern.en import lexeme\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "config['gpu'] = 1\n",
    "# importlib.reload(sys.modules['config'])\n",
    "config['file_name'] = 'testing_paraphrasing.txt'\n",
    "open(config['file_name'], \"w\").close()\n",
    "\n",
    "from utils import *\n",
    "print(config['operation'])\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/m25dehgh/simplification/Edit-Unsup-TS/src\n",
      "/home/m25dehgh/anaconda3/envs/simpenv38/bin/python\n",
      "Reading lines...\n",
      "loading Wikilarge data\n",
      "Read 296402 train sentence pairs\n",
      "Read 1999 valid sentence pairs\n",
      "Read 358 test sentence pairs\n",
      "Read 237052 unique train sentence pairs\n",
      "Read 1999 unique valid sentence pairs\n",
      "Read 358 unique test sentence pairs\n",
      "Trimmed to 296402 train sentence pairs\n",
      "Trimmed to 1999 valid sentence pairs\n",
      "Trimmed to 358 test sentence pairs\n",
      "Trimmed to 237052 unique train sentence pairs\n",
      "Trimmed to 1999 unique valid sentence pairs\n",
      "Trimmed to 358 unique test sentence pairs\n",
      "Loading/Building vocabulary\n",
      "outputword2count (Vocab) file present\n",
      "Generating tf-idf file using simple sentences from the training set\n",
      "Calculating unigram probabilities\n",
      "OOV count\n",
      "1213\n",
      "Total vocabulary size:\n",
      "simple 35650\n"
     ]
    }
   ],
   "source": [
    "%cd ~/simplification/Edit-Unsup-TS/src/\n",
    "! which python\n",
    "\n",
    "from model.structural_decoder import DecoderGRU\n",
    "\n",
    "\n",
    "idf, unigram_prob, output_lang, tag_lang, dep_lang, train_simple, valid_simple, test_simple, train_complex, valid_complex, test_complex, output_embedding_weights, tag_embedding_weights, dep_embedding_weights = prepareData(config['embedding_dim'],\n",
    "config['freq'], config['ver'], config['dataset'], config['operation'])\n",
    "\n",
    "\n",
    "\n",
    "lm_forward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)\n",
    "lm_backward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sys.modules['config'])\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this town 2 percent of people are fucked up'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = \"in this town 2 % of people are fucked up\"\n",
    "ss = ss.replace(\"%\", \"percent\")\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ls': 0, 'dl': 0, 'las': 0, 'rl': 0, 'par': 0}\n",
      "Getting candidates for iteration:  0\n",
      "neg constraints:  feature features featuring featured improve improves improving improved battery batteries batterying batteried option options optioning optioned\n",
      "input:  The Kindle 2 features 16level grayscale display improved battery life 20 percent faster pagerefreshing a texttospeech option to read the text aloud and overall thickness reduced from 08 to 036 inches 91 millimeters.\tfeature|features|featuring|featured|improve|improves|improving|improved|battery|batteries|batterying|batteried|option|options|optioning|optioned\t\n",
      "new:  The Kindle 2 properties of the 16-level grayscale display enhanced the life of the battalion by 20 percent faster pagerefreshing the ability to read the text aloud and overall thickness reduced from 08 to 036 inches 91 mm.\n",
      "\n",
      "Candidate: the kindle 2 properties of the 16 - level grayscale display enhanced the life of the battalion by 20 percent faster pagerefreshing the ability to read the text aloud and overall thickness reduced from 08 to 036 inches 91 mm .\n",
      "Old Prob: 0.04524141550064087, New Sent Prob: 0.05052262544631958\n",
      "Getting candidates for iteration:  1\n",
      "neg constraints:  property properties propertying propertied enhance enhances enhancing enhanced battalion battalions battalioning battalioned display displays displaying displayed\n",
      "input:  the kindle 2 properties of the 16  level grayscale display enhanced the life of the battalion by 20 percent faster pagerefreshing the ability to read the text aloud and overall thickness reduced from 08 to 036 inches 91 mm .\tproperty|properties|propertying|propertied|enhance|enhances|enhancing|enhanced|battalion|battalions|battalioning|battalioned|display|displays|displaying|displayed\t\n",
      "new:  Kindle 2 characteristics of the 16 level grayscale screen improved the life of the battery by 20% faster page refreshing the ability to read the text aloud and overall thickness reduced from 08 to 036 inches 91 mm.\n",
      "\n",
      "Candidate: kindle 2 characteristics of the 16 level grayscale screen improved the life of the battery by 20 % faster page refreshing the ability to read the text aloud and overall thickness reduced from 08 to 036 inches 91 mm .\n",
      "Old Prob: 0.05052262544631958, New Sent Prob: 0.04507744312286377\n",
      "Input complex sentence\n",
      "the kindle 2 features 16-level grayscale display, improved battery life, 20 percent faster page-refreshing, a text-to-speech option to read the text aloud, and overall thickness reduced from 0.8 to 0.36 inches (9.1 millimeters).\n",
      "Reference sentence\n",
      "the kindle 2 features several technological advances.\n",
      "Simplified sentence\n",
      "the kindle 2 properties of the 16 - level grayscale display enhanced the life of the battalion by 20 percent faster pagerefreshing the ability to read the text aloud and overall thickness reduced from 08 to 036 inches 91 mm .\n",
      "\n",
      "\n",
      "Average sentence level SARI till now for sentences\n",
      "0.38113064830018856\n",
      "0.1916392993979201\n",
      "0.9517526455026455\n",
      "0.0\n",
      "Average sentence level BLEU till now for sentences\n",
      "0.03333168744598931\n",
      "Average perplexity of sentences\n",
      "0.05052262544631958\n",
      "Average sentence level FKGL and FRE till now for sentences\n",
      "15.05076923076923\n",
      "56.61923076923078\n",
      "\n",
      "\n",
      "1\n",
      "Runtime of the program is 23.408804655075073\n",
      "total paraphrasing calls 2, total beam calls 2\n",
      "{'ls': 0, 'dl': 0, 'las': 0, 'rl': 0, 'par': 1}\n"
     ]
    }
   ],
   "source": [
    "import tree_edits_beam\n",
    "importlib.reload(sys.modules['tree_edits_beam'])\n",
    "from tree_edits_beam import *\n",
    "cmplx = [\"below are some useful links to facilitate your involvement .\"]\n",
    "simp = [\"below are some useful links to to help you get involved .\"]\n",
    "# cmplx = ['this was demonstrated in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# simp = ['this was shown in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# cmplx = [\"one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.\"]\n",
    "# simp = [\"On one side of the conflicts are the Sudanese military and the Janjaweed, a Sudanese militia group.  They are mostly recruited from the Afro-Arab Abbala tribes.\"] \n",
    "# cmplx = [\"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"]\n",
    "cmplx = ['The Kindle 2 features 16-level grayscale display, improved battery life, 20 percent faster page-refreshing, a text-to-speech option to read the text aloud, and overall thickness reduced from 0.8 to 0.36 inches (9.1 millimeters).']\n",
    "simp = ['The Kindle 2 features several technological advances.']\n",
    "\n",
    "\n",
    "cmplx = ['before the advent of the pocket calculator  it was the most commonly used calculation tool in science and engineering .']\n",
    "simp = ['It was the most commonly used calculation tool before the invention of the pocket calculator.']\n",
    "\n",
    "cmplx = ['The Kindle 2 features 16-level grayscale display, improved battery life, 20 percent faster page-refreshing, a text-to-speech option to read the text aloud, and overall thickness reduced from 0.8 to 0.36 inches (9.1 millimeters).']\n",
    "simp = ['The Kindle 2 features several technological advances.']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# cmplx[0] = all_norms(cmplx[0])\n",
    "\n",
    "sample(cmplx, simp, output_lang, tag_lang, dep_lang, lm_forward, lm_backward, output_embedding_weights, idf, unigram_prob, start)\n",
    "\n",
    "# end = time.time()\n",
    "# print(f\"Runtime of the program is {end - start}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DebertaForSequenceClassification, Trainer, TrainingArguments, DebertaTokenizerFast\n",
    "\n",
    "root = \"/home/m25dehgh/simplification/complex-classifier\"\n",
    "model_name = \"newsela-auto-high-quality\"\n",
    "path_model = root + '/results' + '/' + model_name + \"/whole-high-quality/checkpoint-44361/\"\n",
    "comp_simp_class_model = DebertaForSequenceClassification.from_pretrained(path_model)\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kindle 2 characteristics 16 levels of grayscale enhanced 20 faster page refurbishment of text for speech reading the text aloud and overall thickness reduced from 08 to 036 inches of lrb 91 mm rrb .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser('http://localhost:9000')\n",
    "\n",
    "\n",
    "# sent = 'one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.'\n",
    "# sent = \"Jeddah is the principal gateway to Mecca, Islam's holiest city, which able-bodied Muslims are required to visit at least once in their lifetime.\"\n",
    "# sent = \"Since 2000, the recipient of the Kate Greenaway Medal has also been presented with the Colin Mears Award to the value of £5000.\"\n",
    "sent = \"It was originally thought that the debris thrown up by the collision filled in the smaller craters.\"\n",
    "# sent = \"The Great Dark Spot is thought to represent a hole in the methane cloud deck of Neptune.\" \n",
    "# sent = \"The great dark spot is thought to be a hole in the methane cloud deck of neptune\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"one side of the armed conflict consists mainly of the sudanese army and the janjaweed of one of the sudanese militia groups .\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"\n",
    "# sent = 'They are culturally akin to the coastal peoples of Papua New Guinea.'\n",
    "# sent = 'this was demonstrated in the miller-urey experiment by stanley Miller and Harold in 1953 .'\n",
    "# sent = 'this was shown in the Miller - Urey experiment by stanley Miller and Harold in 1953 .'\n",
    "sent = 'Kindle 2 characteristics 16 levels of grayscale enhanced 20 faster page refurbishment of text for speech reading the text aloud and overall thickness reduced from 08 to 036 inches of lrb 91 mm rrb.'\n",
    "\n",
    "sent = all_norms(sent)\n",
    "tree = next(parser.raw_parse(sent))\n",
    "sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_out(model, tokenizer, sent):\n",
    "    \"returns a dict contaiting : attention mat for all layers, tokens of the input sent, complexity probability\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    toks = tokenizer(text = sent, truncation=True, padding=True, max_length = 100, return_tensors='pt')\n",
    "    \n",
    "    input_ids = toks['input_ids'].to(device)\n",
    "    attention_mask = toks['attention_mask'].to(device)\n",
    "    token_type_ids=toks['token_type_ids'].to(device)\n",
    "    output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_attentions = True, return_dict = True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "    attention = output.attentions\n",
    "    \n",
    "    out = {\"attention\": attention, \"tokens\": tokens, \"prob\": output.logits.squeeze().softmax(dim=0)[1].item()}\n",
    "    \n",
    "    return out\n",
    "\n",
    "def comp_extract(sent, comp_simp_class_model, tokenizer):\n",
    "    'extracting complex tokens from'\n",
    "    \n",
    "    out = get_model_out(comp_simp_class_model, tokenizer, sent)\n",
    "    attention = out['attention']\n",
    "    tokens = out['tokens']\n",
    "    prob = out[\"prob\"]\n",
    "\n",
    "    layer = 1\n",
    "    CLS_attended_tokens = attention[layer].sum(dim=1)[0][0].topk(10)\n",
    "    # [tokens[i] if for i in CLS_attended_tokens[1]], attention[layer].sum(dim=1)[0][0].topk(10)[0]\n",
    "\n",
    "    more_than_thresh = []\n",
    "    less_than_thresh = []\n",
    "    thresh = attention[layer].sum(dim=1)[0][0].mean() * 3/2\n",
    "\n",
    "    for i in range(len(CLS_attended_tokens[0])):\n",
    "        if CLS_attended_tokens[0][i] > thresh:\n",
    "            more_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "        else:\n",
    "            less_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "    \n",
    "    extracted_comps = {\"comp_toks\": more_than_thresh, \n",
    "                       \"not_comp_toks\": less_than_thresh,\n",
    "                       \"threshold\": thresh.item(),\n",
    "                       \"attention\": attention,\n",
    "                       'tokens': tokens,\n",
    "                       'prob': prob,\n",
    "                      }\n",
    "    return extracted_comps\n",
    "\n",
    "\n",
    "def word_from_toks(comp_toks, tokens):\n",
    "    '''returns words for negative constraints'''\n",
    "\n",
    "    # maximum number of accepted negative constraints\n",
    "    max_num_accepted_consts = 4\n",
    "    negs = []\n",
    "    special_toks = ['[SEP]', '[CLS]', '.', 'Ġ.']\n",
    "    \n",
    "    for tok in comp_toks:\n",
    "        \n",
    "        # Each token should be a word, not a part of word\n",
    "        if tok[0] == 'Ġ' and tok not in special_toks:\n",
    "        \n",
    "            # first word is usually selected mistakably so we do not pass it to the paraphraser\n",
    "            if tokens.index(tok) + 1 != len(tokens) and tokens.index(tok) != 1: \n",
    "        \n",
    "                # We want the token be single word, not a starting part of a word\n",
    "                if tokens[tokens.index(tok) + 1][0] == 'Ġ':\n",
    "                    negs.append(tok[1:])\n",
    "\n",
    "    new_neg = []\n",
    "    # adding all words with similar root \n",
    "    for tok in negs[:max_num_accepted_consts]:\n",
    "        new_neg += lexeme(tok)\n",
    "    \n",
    "    return new_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1.25}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'testing_paraphrasing.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': True, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.7, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.5, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexeme handled\n",
      "neg constraints:  debri debris debriing debried collision collisions collisioning collisioned\n",
      "input:  it was originally thought that the debris thrown up by the collision filled in the smaller craters .\tdebri|debris|debriing|debried|collision|collisions|collisioning|collisioned\t\n",
      "new:  Originally, it was thought that the rubble thrown up by the crash filled with smaller craters.\n",
      "\n",
      "originally , it was thought that the rubble thrown up by the crash filled with smaller craters .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(sent)\n",
    "entits = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entits.append(ent.text)\n",
    "    print(ent.text)\n",
    "    \n",
    "print(paraph(sent, \"\", entits, rest_pos_const=False))\n",
    "\n",
    "\n",
    "\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.']\n",
      "===========\n",
      "['neptune']\n",
      "===========\n",
      "['neptune']\n",
      "===========\n",
      "['of']\n",
      "===========\n",
      "['of', 'neptune']\n",
      "===========\n",
      "['deck']\n",
      "===========\n",
      "['cloud']\n",
      "===========\n",
      "['methane']\n",
      "===========\n",
      "['the']\n",
      "===========\n",
      "['the', 'methane', 'cloud', 'deck']\n",
      "===========\n",
      "['the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['in']\n",
      "===========\n",
      "['in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['hole']\n",
      "===========\n",
      "['a']\n",
      "===========\n",
      "['a', 'hole']\n",
      "===========\n",
      "['represent']\n",
      "===========\n",
      "['represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['to']\n",
      "===========\n",
      "['to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['thought']\n",
      "===========\n",
      "['thought', 'to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['is']\n",
      "===========\n",
      "['is', 'thought', 'to', 'represent', 'a', 'hole', 'in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune']\n",
      "===========\n",
      "['spot']\n",
      "===========\n",
      "['dark']\n",
      "===========\n",
      "['great']\n",
      "===========\n",
      "['the']\n",
      "===========\n",
      "['the', 'great', 'dark', 'spot']\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pos) - 1, 1, -1):\n",
    "    if not isinstance(tree[pos[i]], str):\n",
    "        print(tree[pos[i]].leaves())\n",
    "        print(\"===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1c43965fc138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreepositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "pos = tree.treepositions()\n",
    "len(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phrase_tags = ['S', 'ADJP', 'ADVP', 'CONJP', 'FRAG', 'INTJ', 'LST', 'NAC', 'NP', 'NX', 'PP', 'PRN', 'PRT',\n",
    "               'QP', 'RRC', 'UCP', 'VP', 'WHADJP', 'WHAVP', 'WHNP', 'WHPP', 'X', 'SBAR']\n",
    "p = []\n",
    "pos = tree.treepositions()\n",
    "for i in range(len(pos) - 1, 1, -1):\n",
    "    if not isinstance(tree[pos[i]], str):\n",
    "        if tree[pos[i]].label() in phrase_tags:\n",
    "            p.append(tree[pos[i]].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['neptune'],\n",
       " ['of', 'neptune'],\n",
       " ['the', 'methane', 'cloud', 'deck'],\n",
       " ['the', 'methane', 'cloud', 'deck', 'of', 'neptune'],\n",
       " ['in', 'the', 'methane', 'cloud', 'deck', 'of', 'neptune'],\n",
       " ['a', 'hole'],\n",
       " ['represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['thought',\n",
       "  'to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['is',\n",
       "  'thought',\n",
       "  'to',\n",
       "  'represent',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'methane',\n",
       "  'cloud',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'neptune'],\n",
       " ['the', 'great', 'dark', 'spot']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in p:\n",
    "#     # print(\"p[i] is :\", i)\n",
    "#     print(\"out put for dlt :\", delete_leaves(sent, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', 'useful', 'links', 'to', 'facilitate', 'your', 'involvement']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9b9e9d5a5d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "i =8\n",
    "if 2 <= len(p[i]) <= 7:\n",
    "    print(p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ['to',\n",
    "  'represent',\n",
    "  'a',\n",
    "  'hole',\n",
    "  'in',\n",
    "  'the',\n",
    "  'methane',\n",
    "  'cloud',\n",
    "  'deck',\n",
    "  'of',\n",
    "  'neptune']\n",
    "a = ['to',\n",
    "  'represent',\n",
    "  'a',\n",
    "  'hole',\n",
    "  'in',\n",
    "  'the',\n",
    "  'methane',\n",
    "  'cloud',\n",
    "  'deck',\n",
    "  'of',\n",
    "  'neptune']\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(sent)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def const_paraph(sent, neg_const, rest_pos_const=False):\n",
    "    stp_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    neg_const = neg_const.split(\" \")\n",
    "    entities = get_entities(sent)\n",
    "    pos_const = []\n",
    "\n",
    "    neg_const = [x for x in neg_const if x not in entities and x not in stp_words]\n",
    "\n",
    "    if len(neg_const) >= 10:\n",
    "        return -1\n",
    "\n",
    "    if rest_pos_const:\n",
    "        for i in sent.split():\n",
    "            if i not in neg_const and i not in stp_words:\n",
    "                pos_const.append(i.lower())\n",
    "    else:\n",
    "        pos_const = entities\n",
    "\n",
    "    inp = sent + '.' + \"\\t\" + \"|\".join(neg_const) + '\\t' + \"|\".join(pos_const)\n",
    "\n",
    "    print(\"input: \", inp)\n",
    "\n",
    "    f = open(\"inp_par.txt\", \"w\")\n",
    "    f.write(inp)\n",
    "    f.close()\n",
    "\n",
    "    f = open(\"out_par.txt\", \"w\")\n",
    "    f.close()\n",
    "\n",
    "    #TODO\n",
    "    imr_dir_path = '/home/m25dehgh/simplification/improved-ParaBank-rewriter'\n",
    "    bashCommand = f\"{imr_dir_path}/paraphrase.sh < ./inp_par.txt > ./out_par.txt \"\n",
    "\n",
    "    os.system(bashCommand)\n",
    "\n",
    "    f = open(\"out_par.txt\", \"r\")\n",
    "    return f.read()\n",
    "\n",
    "# def cons_paraphrase(sent, leaves):\n",
    "def paraph(sent, leaves, rest_pos_const=False):\n",
    "#     phrase = ''\n",
    "    # print(leaves)\n",
    "#     for i in range(len(leaves)):\n",
    "#         phrase = phrase + ' ' + leaves[i]\n",
    "#     phrase = phrase + ' '\n",
    "#     old = sent\n",
    "    # print(\"deleting\", phrase)\n",
    "\n",
    "    sent = const_paraph(sent, \" \".join(leaves), rest_pos_const)\n",
    "\n",
    "#     print('new: ', sent)\n",
    "    return correct(all_norms(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  below are some useful links to facilitate your involvement .\tuseful|links|facilitate|involvement\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'below are some helpful references to make it easier for you to participate .'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraph(sent, p[4], rest_pos_const=False)\n",
    "# sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since 2000  the recipient of the kate greenaway medal has also been presented with the colin mears award to the value of £ 5000 .\tpresented|value\tsince|2000|recipient|kate|greenaway|medal|also|colin|mears|award|£|5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'since 2000 kate greenaway medal recipient also award colin mears 5000 <unk>.\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_paraph(all_norms(sent), \"been presented to the value of 5000\", rest_pos_const=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phrase = ''\n",
    "# print(leaves)\n",
    "for i in range(len(p[12])):\n",
    "    phrase = phrase + ' ' + p[12][i]\n",
    "phrase = phrase + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a sudanese militia group'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(p[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a sudanese militia group '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpenv38",
   "language": "python",
   "name": "simpenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
