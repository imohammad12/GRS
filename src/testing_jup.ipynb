{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': False, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n",
      "sample\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['config'])\n",
    "from config import model_config as config\n",
    "import spacy\n",
    "from pattern.en import lexeme\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "config['gpu'] = 1\n",
    "# importlib.reload(sys.modules['config'])\n",
    "config['file_name'] = 'testing_paraphrasing.txt'\n",
    "open(config['file_name'], \"w\").close()\n",
    "\n",
    "from utils import *\n",
    "print(config['operation'])\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/m25dehgh/simplification/Edit-Unsup-TS/src\n",
      "/home/m25dehgh/anaconda3/envs/simpenv38/bin/python\n",
      "Reading lines...\n",
      "loading Wikilarge data\n",
      "Read 296402 train sentence pairs\n",
      "Read 1999 valid sentence pairs\n",
      "Read 358 test sentence pairs\n",
      "Read 237052 unique train sentence pairs\n",
      "Read 1999 unique valid sentence pairs\n",
      "Read 358 unique test sentence pairs\n",
      "Trimmed to 296402 train sentence pairs\n",
      "Trimmed to 1999 valid sentence pairs\n",
      "Trimmed to 358 test sentence pairs\n",
      "Trimmed to 237052 unique train sentence pairs\n",
      "Trimmed to 1999 unique valid sentence pairs\n",
      "Trimmed to 358 unique test sentence pairs\n",
      "Loading/Building vocabulary\n",
      "outputword2count (Vocab) file present\n",
      "Generating tf-idf file using simple sentences from the training set\n",
      "Calculating unigram probabilities\n",
      "OOV count\n",
      "1213\n",
      "Total vocabulary size:\n",
      "simple 35650\n"
     ]
    }
   ],
   "source": [
    "%cd ~/simplification/Edit-Unsup-TS/src/\n",
    "! which python\n",
    "\n",
    "from model.structural_decoder import DecoderGRU\n",
    "\n",
    "\n",
    "idf, unigram_prob, output_lang, tag_lang, dep_lang, train_simple, valid_simple, test_simple, train_complex, valid_complex, test_complex, output_embedding_weights, tag_embedding_weights, dep_embedding_weights = prepareData(config['embedding_dim'],\n",
    "config['freq'], config['ver'], config['dataset'], config['operation'])\n",
    "\n",
    "\n",
    "\n",
    "lm_forward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)\n",
    "lm_backward = DecoderGRU(config['hidden_size'], output_lang.n_words, tag_lang.n_words, dep_lang.n_words, config['num_layers'],\n",
    "    output_embedding_weights, tag_embedding_weights, dep_embedding_weights, config['embedding_dim'], config['tag_dim'], config['dep_dim'], config['dropout'], config['use_structural_as_standard']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'Wikilarge/output/simplifications.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': True, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.5, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.25, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sys.modules['config'])\n",
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ls': 0, 'dl': 0, 'las': 0, 'rl': 0, 'par': 0}\n",
      "similarity of the two sentences:  tensor(1., device='cuda:0')\n",
      "Getting candidates for iteration:  0\n",
      "neg constraints:  facilitate facilitates facilitating facilitated\n",
      "input:  below are some useful links to facilitate your involvement .\tfacilitate|facilitates|facilitating|facilitated\t\n",
      "new:  Below are some useful links to make it easier for you to participate.\n",
      "\n",
      "similarity of the two sentences:  tensor(0.8501, device='cuda:0')\n",
      "Candidate: below are some useful links to make it easier for you to participate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.1457918882369995 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.9078, device='cuda:0')\n",
      "Candidate: below are some useful links to facilitate .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.04704469442367554 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8438, device='cuda:0')\n",
      "Candidate: below are some useful links to .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.8880649134516716 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "Candidate: below are some useful links .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.8880444020032883 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "Candidate: below are some useful links .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.8880444020032883 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.3400, device='cuda:0')\n",
      "Candidate: below are .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.2834, device='cuda:0')\n",
      "Candidate: below .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.9774, device='cuda:0')\n",
      "Candidate: are some useful links to facilitate your involvement .\n",
      "Old Prob: 0.04782158136367798, New Sent Prob: 0.04986381530761719 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8438, device='cuda:0')\n",
      "Getting candidates for iteration:  1\n",
      "neg constraints:  \n",
      "input:  below are some useful links to .\t\t\n",
      "new:  Below are some useful links.\n",
      "\n",
      "similarity of the two sentences:  tensor(0.8357, device='cuda:0')\n",
      "Candidate: below are some useful links .\n",
      "Old Prob: 0.8880649134516716, New Sent Prob: 0.8880444020032883 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.3717, device='cuda:0')\n",
      "Candidate: below are to .\n",
      "Old Prob: 0.8880649134516716, New Sent Prob: 0 \n",
      "\n",
      "similarity of the two sentences:  tensor(0.8299, device='cuda:0')\n",
      "Candidate: are some useful links to .\n",
      "Old Prob: 0.8880649134516716, New Sent Prob: 0.8880407214164734 \n",
      "\n",
      "Input complex sentence\n",
      "below are some useful links to facilitate your involvement .\n",
      "Reference sentence\n",
      "below are some useful links to to help you get involved .\n",
      "Simplified sentence\n",
      "below are some useful links to .\n",
      "\n",
      "\n",
      "Average sentence level SARI till now for sentences\n",
      "0.6666666666666666\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Average sentence level BLEU till now for sentences\n",
      "0.4116538266387963\n",
      "Average perplexity of sentences\n",
      "0.8880649134516716\n",
      "Average sentence level FKGL and FRE till now for sentences\n",
      "4.450000000000003\n",
      "73.84500000000001\n",
      "\n",
      "\n",
      "1\n",
      "Runtime of the program is 22.69162344932556\n",
      "total paraphrasing calls 2, total beam calls 2\n",
      "{'ls': 0, 'dl': 4, 'las': 0, 'rl': 0, 'par': 1}\n"
     ]
    }
   ],
   "source": [
    "import tree_edits_beam\n",
    "importlib.reload(sys.modules['tree_edits_beam'])\n",
    "from tree_edits_beam import *\n",
    "cmplx = [\"below are some useful links to facilitate your involvement .\"]\n",
    "simp = [\"below are some useful links to to help you get involved .\"]\n",
    "# cmplx = ['this was demonstrated in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# simp = ['this was shown in the miller - urey experiment by stanley l . miller and harold c . urey in 1953 .']\n",
    "# cmplx = [\"one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.\"]\n",
    "# simp = [\"On one side of the conflicts are the Sudanese military and the Janjaweed, a Sudanese militia group.  They are mostly recruited from the Afro-Arab Abbala tribes.\"] \n",
    "# cmplx = [\"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"]\n",
    "# cmplx = ['The Kindle 2 features 16-level grayscale display, improved battery life, 20 percent faster page-refreshing, a text-to-speech option to read the text aloud, and overall thickness reduced from 0.8 to 0.36 inches (9.1 millimeters).']\n",
    "# simp = ['The Kindle 2 features several technological advances.']\n",
    "\n",
    "\n",
    "# cmplx = ['before the advent of the pocket calculator  it was the most commonly used calculation tool in science and engineering .']\n",
    "# simp = ['It was the most commonly used calculation tool before the invention of the pocket calculator.']\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# cmplx[0] = all_norms(cmplx[0])\n",
    "\n",
    "sample(cmplx, simp, output_lang, tag_lang, dep_lang, lm_forward, lm_backward, output_embedding_weights, idf, unigram_prob, start)\n",
    "\n",
    "# end = time.time()\n",
    "# print(f\"Runtime of the program is {end - start}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "below are some useful links to facilitate your involvement . \t\t below are some useful links to to help you get involved . \t\t Score: 0.9086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9086, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_sim(cmplx[0], simp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DebertaForSequenceClassification, Trainer, TrainingArguments, DebertaTokenizerFast\n",
    "\n",
    "root = \"/home/m25dehgh/simplification/complex-classifier\"\n",
    "model_name = \"newsela-auto-high-quality\"\n",
    "path_model = root + '/results' + '/' + model_name + \"/whole-high-quality/checkpoint-44361/\"\n",
    "comp_simp_class_model = DebertaForSequenceClassification.from_pretrained(path_model)\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kindle 2 characteristics 16 levels of grayscale enhanced 20 faster page refurbishment of text for speech reading the text aloud and overall thickness reduced from 08 to 036 inches of lrb 91 mm rrb .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser('http://localhost:9000')\n",
    "\n",
    "\n",
    "# sent = 'one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.'\n",
    "# sent = \"Jeddah is the principal gateway to Mecca, Islam's holiest city, which able-bodied Muslims are required to visit at least once in their lifetime.\"\n",
    "# sent = \"Since 2000, the recipient of the Kate Greenaway Medal has also been presented with the Colin Mears Award to the value of £5000.\"\n",
    "sent = \"It was originally thought that the debris thrown up by the collision filled in the smaller craters.\"\n",
    "# sent = \"The Great Dark Spot is thought to represent a hole in the methane cloud deck of Neptune.\" \n",
    "# sent = \"The great dark spot is thought to be a hole in the methane cloud deck of neptune\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"one side of the armed conflict consists mainly of the sudanese army and the janjaweed of one of the sudanese militia groups .\"\n",
    "# sent = \"below are some useful links to facilitate your involvement .\"\n",
    "# sent = \"none of the authors , contributors , sponsors , administrators , vandals , or anyone else connected with wikipedia , in any way whatsoever , can be responsible for your use of the information contained in or linked from these web pages .\"\n",
    "# sent = 'They are culturally akin to the coastal peoples of Papua New Guinea.'\n",
    "# sent = 'this was demonstrated in the miller-urey experiment by stanley Miller and Harold in 1953 .'\n",
    "# sent = 'this was shown in the Miller - Urey experiment by stanley Miller and Harold in 1953 .'\n",
    "sent = 'Kindle 2 characteristics 16 levels of grayscale enhanced 20 faster page refurbishment of text for speech reading the text aloud and overall thickness reduced from 08 to 036 inches of lrb 91 mm rrb.'\n",
    "\n",
    "sent = all_norms(sent)\n",
    "tree = next(parser.raw_parse(sent))\n",
    "sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_out(model, tokenizer, sent):\n",
    "    \"returns a dict contaiting : attention mat for all layers, tokens of the input sent, complexity probability\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    toks = tokenizer(text = sent, truncation=True, padding=True, max_length = 100, return_tensors='pt')\n",
    "    \n",
    "    input_ids = toks['input_ids'].to(device)\n",
    "    attention_mask = toks['attention_mask'].to(device)\n",
    "    token_type_ids=toks['token_type_ids'].to(device)\n",
    "    output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_attentions = True, return_dict = True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "    attention = output.attentions\n",
    "    \n",
    "    out = {\"attention\": attention, \"tokens\": tokens, \"prob\": output.logits.squeeze().softmax(dim=0)[1].item()}\n",
    "    \n",
    "    return out\n",
    "\n",
    "def comp_extract(sent, comp_simp_class_model, tokenizer):\n",
    "    'extracting complex tokens from'\n",
    "    \n",
    "    out = get_model_out(comp_simp_class_model, tokenizer, sent)\n",
    "    attention = out['attention']\n",
    "    tokens = out['tokens']\n",
    "    prob = out[\"prob\"]\n",
    "\n",
    "    layer = 1\n",
    "    CLS_attended_tokens = attention[layer].sum(dim=1)[0][0].topk(10)\n",
    "    # [tokens[i] if for i in CLS_attended_tokens[1]], attention[layer].sum(dim=1)[0][0].topk(10)[0]\n",
    "\n",
    "    more_than_thresh = []\n",
    "    less_than_thresh = []\n",
    "    thresh = attention[layer].sum(dim=1)[0][0].mean() * 3/2\n",
    "\n",
    "    for i in range(len(CLS_attended_tokens[0])):\n",
    "        if CLS_attended_tokens[0][i] > thresh:\n",
    "            more_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "        else:\n",
    "            less_than_thresh.append(tokens[CLS_attended_tokens[1][i]])\n",
    "    \n",
    "    extracted_comps = {\"comp_toks\": more_than_thresh, \n",
    "                       \"not_comp_toks\": less_than_thresh,\n",
    "                       \"threshold\": thresh.item(),\n",
    "                       \"attention\": attention,\n",
    "                       'tokens': tokens,\n",
    "                       'prob': prob,\n",
    "                      }\n",
    "    return extracted_comps\n",
    "\n",
    "\n",
    "def word_from_toks(comp_toks, tokens):\n",
    "    '''returns words for negative constraints'''\n",
    "\n",
    "    # maximum number of accepted negative constraints\n",
    "    max_num_accepted_consts = 4\n",
    "    negs = []\n",
    "    special_toks = ['[SEP]', '[CLS]', '.', 'Ġ.']\n",
    "    \n",
    "    for tok in comp_toks:\n",
    "        \n",
    "        # Each token should be a word, not a part of word\n",
    "        if tok[0] == 'Ġ' and tok not in special_toks:\n",
    "        \n",
    "            # first word is usually selected mistakably so we do not pass it to the paraphraser\n",
    "            if tokens.index(tok) + 1 != len(tokens) and tokens.index(tok) != 1: \n",
    "        \n",
    "                # We want the token be single word, not a starting part of a word\n",
    "                if tokens[tokens.index(tok) + 1][0] == 'Ġ':\n",
    "                    negs.append(tok[1:])\n",
    "\n",
    "    new_neg = []\n",
    "    # adding all words with similar root \n",
    "    for tok in negs[:max_num_accepted_consts]:\n",
    "        new_neg += lexeme(tok)\n",
    "    \n",
    "    return new_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip': 50, 'lr': 0.001, 'num_steps': 87, 'threshold': {'ls': 0.8, 'dl': 1, 'las': 3.0, 'rl': 1.25, 'par': 1.25}, 'epochs': 100, 'set': 'test', 'lm_name': 'Wikilarge/structured_lm_forward_300_150_0_4_freq5', 'use_structural_as_standard': False, 'lm_backward': False, 'embedding_dim': 300, 'tag_dim': 150, 'dep_dim': 150, 'hidden_size': 256, 'num_layers': 2, 'freq': 0, 'min_length': 100, 'dataset': 'Wikilarge', 'ver': 'glove.6B.', 'dropout': 0.4, 'batch_size': 64, 'print_every': 100, 'MAX_LENGTH': 85, 'double_LM': False, 'gpu': 1, 'awd': False, 'file_name': 'testing_paraphrasing.txt', 'fre': True, 'SLOR': True, 'beam_size': 1, 'elmo': False, 'min_length_of_edited_sent': 6, 'lexical_simplification': False, 'constrained_paraphrasing': True, 'delete_leaves': True, 'leaves_as_sent': False, 'reorder_leaves': False, 'check_min_length': True, 'cos_similarity_threshold': 0.7, 'cos_value_for_synonym_acceptance': 0.5, 'min_idf_value_for_ls': 11, 'sentence_probability_power': 1.0, 'named_entity_score_power': 1.0, 'len_power': 0.5, 'fre_power': 1.0, 'operation': 'sample'}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexeme handled\n",
      "neg constraints:  debri debris debriing debried collision collisions collisioning collisioned\n",
      "input:  it was originally thought that the debris thrown up by the collision filled in the smaller craters .\tdebri|debris|debriing|debried|collision|collisions|collisioning|collisioned\t\n",
      "new:  Originally, it was thought that the rubble thrown up by the crash filled with smaller craters.\n",
      "\n",
      "originally , it was thought that the rubble thrown up by the crash filled with smaller craters .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(sent)\n",
    "entits = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entits.append(ent.text)\n",
    "    print(ent.text)\n",
    "    \n",
    "print(paraph(sent, \"\", entits, rest_pos_const=False))\n",
    "\n",
    "\n",
    "\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phrase_tags = ['S', 'ADJP', 'ADVP', 'CONJP', 'FRAG', 'INTJ', 'LST', 'NAC', 'NP', 'NX', 'PP', 'PRN', 'PRT',\n",
    "               'QP', 'RRC', 'UCP', 'VP', 'WHADJP', 'WHAVP', 'WHNP', 'WHPP', 'X', 'SBAR']\n",
    "p = []\n",
    "pos = tree.treepositions()\n",
    "for i in range(len(pos) - 1, 1, -1):\n",
    "    if not isinstance(tree[pos[i]], str):\n",
    "        if tree[pos[i]].label() in phrase_tags:\n",
    "            p.append(tree[pos[i]].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since 2000  the recipient of the kate greenaway medal has also been presented with the colin mears award to the value of £ 5000 .\tpresented|value\tsince|2000|recipient|kate|greenaway|medal|also|colin|mears|award|£|5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'since 2000 kate greenaway medal recipient also award colin mears 5000 <unk>.\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_paraph(all_norms(sent), \"been presented to the value of 5000\", rest_pos_const=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpenv38",
   "language": "python",
   "name": "simpenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
