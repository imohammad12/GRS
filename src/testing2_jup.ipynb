{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7fbf805bd1c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%cd asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%cd /home/m25dehgh/simplification/Edit-Unsup-TS/src/asset\n",
    "!/home/m25dehgh/anaconda3/envs/easenv/bin/easse evaluate --orig_sents_path asset.test.orig.2 --sys_sents_path asset.handgenerated --refs_sents_paths 'asset.testing.simp' -t custom -m 'sari,bleu' -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective 28\n",
    "# non-effective 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import normalizing\n",
    "print('one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed , a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan .')\n",
    "print(normalizing.all_norms('one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed , a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser(url='http://localhost:9000')\n",
    "con_tree = list(parser.parse('one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed, a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan.'.split()))\n",
    "con_tree = list(parser.parse('the great dark spot is thought to represent a hole in the methane cloud deck of neptune.'.split()))\n",
    "con_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n",
    "for i in con_tree:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw.tree import TreeView\n",
    "t = Tree.fromstring('(S (NP this tree) (VP (V is) (AdjP pretty)))')\n",
    "# t = con_tree[0]\n",
    "TreeView(t)._cframe.print_to_file('output.ps')\n",
    "os.system('convert output.ps output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "from nltk.draw.util import CanvasFrame\n",
    "from nltk.draw import TreeWidget\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "sentence = 'I saw a dog'\n",
    "# parser = Parser()\n",
    "parsed = parser.parse(sentence)\n",
    "cf = CanvasFrame()\n",
    "tc = TreeWidget(cf.canvas(),parsed)\n",
    "cf.add_widget(tc,10,10) # (10,10) offsets\n",
    "cf.print_to_file('tree.ps')\n",
    "cf.destroy()\n",
    "! convert tree.ps tree.png\n",
    "! rm tree.ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(con_tree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /home/m25dehgh/simplification/Edit-Unsup-TS/\n",
    "\n",
    "# wiki_orig_out = open('src/Wikilarge/output/RM+EX+LS+RO.txt', encoding='utf-8').read().split('\\n')\n",
    "root_path = '/home/m25dehgh/simplification/controllable-simplification/src/'\n",
    "wiki_me_out = open(root_path + 'Wikilarge/output/simplifications.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "wiki_me_out \n",
    "\n",
    "new_wiki = []\n",
    "for i in range(len(wiki_me_out)):\n",
    "    if i % 8 == 2:\n",
    "        new_wiki.append(wiki_me_out[i])\n",
    "\n",
    "# new_wiki = new_wiki[-359:]\n",
    "        \n",
    "with open(root_path + '/Wikilarge/output/dl:false-par:1-asset-comp-simp-class.txt', \"w\") as file:\n",
    "    for i in range(len(new_wiki)):\n",
    "        file.write(new_wiki[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a string can vibrate in different ways and each form .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_wiki[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsela_test_dst = []\n",
    "for i in range(10):\n",
    "    newsela_test_dst.append(open('/home/m25dehgh/simplification/datasets/asset/dataset/asset.test.simp.{}'.format(i), encoding='utf-8').read().split('\\n'))\n",
    "\n",
    "    newsela_test_dst[i] = [normalizing.all_norms(j) for j in newsela_test_dst[i]]\n",
    "\n",
    "\n",
    "    with open('/home/m25dehgh/simplification/datasets/asset/dataset/asset.test.norm.simp.{}'.format(i), \"w\") as file:\n",
    "        for j in range(len(newsela_test_dst[i])):\n",
    "            file.write(newsela_test_dst[i][j] + \"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsela_test_dst = open('/home/m25dehgh/simplification/datasets/asset/dataset/asset.test.orig', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "newsela_test_dst = [normalizing.all_norms(j) for j in newsela_test_dst]\n",
    "\n",
    "\n",
    "with open('/home/m25dehgh/simplification/datasets/asset/dataset/asset.test.norm.orig', \"w\") as file:\n",
    "    for j in range(len(newsela_test_dst)):\n",
    "        file.write(newsela_test_dst[j] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_src = open('/home/m25dehgh/simplification/Edit-Unsup-TS/src/turkcorpus/test.8turkers.tok.norm', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(f,s,model = 12):\n",
    "    sum = 0\n",
    "    for c in f:\n",
    "        sum += int(c)\n",
    "    for c in s:\n",
    "        sum += int(c)\n",
    "    return -sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from queue import PriorityQueue\n",
    "# pq = PriorityQueue() \n",
    "import heapq\n",
    "\n",
    "pq = [] # Priority Queue\n",
    "indx = ['0','1','2','3','4','5']\n",
    "model = 12\n",
    "beam_size = 3\n",
    "used = {}\n",
    "for i in range(len(indx)):\n",
    "    for j in range(i+1, len(indx)):\n",
    "        heapq.heappush(pq, (score(indx[i],indx[j],model), indx[i] + indx[j]))\n",
    "        heapq.heappush(pq, (score(indx[j],indx[i],model), indx[j] + indx[i]))\n",
    "\n",
    "        used.update({indx[i] + indx[j]: score(indx[i],indx[j],model)})\n",
    "        used.update({indx[j] + indx[i]: score(indx[j],indx[i],model)})\n",
    "\n",
    "\n",
    "bool_break = False\n",
    "final_seq = ''\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "#     best_seq = heapq.nsmallest(beam_size,pq, key=lambda s: s[0])\n",
    "\n",
    "    best_seq = []\n",
    "    for beam in range(beam_size):\n",
    "        pair = heapq.heappop(pq)\n",
    "        best_seq.append(pair)\n",
    "        sequence = pair[1]\n",
    "        if len(sequence) == 6:\n",
    "            final_seq = pair\n",
    "            bool_break = True\n",
    "    \n",
    "\n",
    "\n",
    "    if bool_break:\n",
    "        break\n",
    "\n",
    "        \n",
    "    for pair in best_seq:\n",
    "        sent_str = pair[1]\n",
    "        indx = ['0','1','2','3','4','5']\n",
    "        \n",
    "        # Remove existing sentences in the current sequence in order to not be included in the future sequences\n",
    "        for car in sent_str:\n",
    "            indx.remove(car)\n",
    "            \n",
    "        for new_sent in indx:\n",
    "            cand1 = sent_str + new_sent\n",
    "            cand2 = new_sent + sent_str\n",
    "            \n",
    "            if cand1 not in used:\n",
    "                score_cand1 = score(sent_str, new_sent, model)\n",
    "                used.update({cand1: score_cand1})\n",
    "                heapq.heappush(pq, (score_cand1, cand1))\n",
    "\n",
    "            if cand2 not in used:\n",
    "                score_cand2 = score(new_sent, sent_str, model)\n",
    "                used.update({cand2: score_cand2})\n",
    "                heapq.heappush(pq, (score_cand2, cand2))\n",
    "             \n",
    "        \n",
    "final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the spearman's correlation between two variables\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from scipy.stats import spearmanr\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "# prepare data\n",
    "data1 = rand(10) * 20\n",
    "data2 = data1 + (rand(10) * 10)\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(data1, data2)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "data1 = [4,2,3,5,1]\n",
    "data2 = [1,3,2,4,5]\n",
    "spearmanr(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq.heappop(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import heapq\n",
    "kk = [1,12,3,8,15,20,2,100]\n",
    "# heapq.heapify(kk)\n",
    "# a.sort()\n",
    "print(heapq.heappop(kk))\n",
    "# kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '123'\n",
    "for i in a:\n",
    "    print(i == '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpenv38",
   "language": "python",
   "name": "simpenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
