#Newsela -> {'ls':1.25, 'dl':1.25, 'las':1.25, 'rl':1.25, 'pa': 1.25}, Wikilarge ->{'ls':0.8, 'dl':1.25, 'las':5.0, 'rl':1.25, 'pa': 1.25}
# "threshold": {'ls':0.8, 'dl':1.25, 'las':5.0, 'rl':1.25, 'pa': 1.25},
# changed
# 'threshold': {'ls':1.25, 'dl':1.25, 'las':1.25, 'rl':1.25, 'pa': 1.25},  # For Newsela
# 'threshold': {'ls': 1.25, 'dl': 1, 'las': 0.75, 'rl':1.25, 'pa': 1.25},  # For ASSET 37.3 with ls and ro
# 'threshold': {'ls': 0.75, 'dl': 1.35, 'las': 3.0, 'rl':1.5, 'pa': 1.25},  # For ASSET 36.02 with out ls and ro
# 'threshold': {'ls': 1.25, 'dl': 1, 'las': 0.75, 'rl':1.25, 'pa': 1.25},  # For ASSET 37.25 without ls and ro
# 'threshold': {'ls': .8, 'dl': 1, 'las': 3.0, 'rl':1.25, 'pa': 1.25},  # For ASSET 32.60 only ls
# 'threshold': {'ls': .8, 'dl': 1, 'las': 3.0, 'rl':1.25, 'pa': 1.25},  # For ASSET 37.36 only LS+RM
# 'lm_name': 'Newsela/structured_lm_forward_300_150_0_4', #wikilarge -> Wikilarge/structured_lm_forward_300_150_0_4_freq5, newsela -> Newsela/structured_lm_forward_300_150_0_4
"dataset": 'Wikilarge',  # 'Wikilarge', #Wikilarge, Newsela  #  changed
'operation': 'sample', # or sample or train_lm,
"orig_file_path": "/home/m25dehgh/simplification/datasets/asset/dataset/asset.test.norm.orig",

"config['simplicity_threshold']": Union[bool, str, float]
    False : similarity threshold will not be used in calculating score.

    ====== Delete =========
    "old_sim" : old version of similarity threshold will be used. Should not be used with new score function.
    ====== Delete =========

    float : similarity threshold used for new score function. Should not be used with old score function

====== Delete =========
"config['score_function']" : str
    "new": using new score function
    "old": using old score function
====== Delete =========

  "log_directory": "/home/m25dehgh/simplification/outputs/asset/whole-dataset",
  "ref_folder_path": "/home/m25dehgh/simplification/datasets/asset-from-easse/ref-test",
  "orig_file_path": "/home/m25dehgh/simplification/datasets/asset-from-easse/asset.test.orig",
  "extra_log_directory": "/home/m25dehgh/simplification/outputs/newsela/whole-dataset",

  "log_directory": "/home/m25dehgh/simplification/outputs/newsela/whole-dataset",
  "ref_folder_path": "/home/m25dehgh/simplification/datasets/newsela/dhruv-newsela/ref-test-orig",
  "orig_file_path": "/home/m25dehgh/simplification/datasets/newsela/dhruv-newsela/V0V4_V1V4_V2V4_V3V4_V0V3_V0V2_V1V3.aner.ori.test.src",
  "extra_log_directory": "/home/m25dehgh/simplification/outputs/asset/whole-dataset",

  "ref_folder_path": "/home/m25dehgh/simplification/outputs/cleanup-test/ref-folder",
  "orig_file_path": "/home/m25dehgh/simplification/outputs/cleanup-test/input_newsela_test.txt",

 "config['paraphrasing_model']" : str
    "imr" : using the original paraphrasing model that was a simple transformer trained on Parabank without
     using any pretrained language model.

     else --> hugging face model that is fine-tuned on paraphrasing datasets (Parabank2 or other datasets).
     "tuner007/pegasus_paraphrase"
     "/home/m25dehgh/simplification/testing-notebooks/bart-large-mnli-finetuned-parabank2-selected/checkpoint-5500"

 "config['grammar_model']"
    :"/home/m25dehgh/simplification/grammar-checker/results/deberta-base-cola/checkpoint-716",
